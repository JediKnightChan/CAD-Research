{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating merged dataframe with genetical score\n",
    "main_df_n = pd.read_csv('GWAS_clinical.csv')\n",
    "score_df_n = pd.read_csv('plink_001.csv', delim_whitespace=True)\n",
    "res_df_n = pd.merge(main_df_n, score_df_n, left_on='FamID', right_on='FID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "df = res_df_n.dropna()\n",
    "\n",
    "# Genetical and non-genetical factors\n",
    "df_data = df.loc[:, ['sex', 'tg', 'hdl', 'ldl', 'SCORESUM']]\n",
    "df_target = df.loc[:, 'CAD']\n",
    "\n",
    "# Only genetical factors\n",
    "df_gen_data = df.loc[:, ['SCORESUM']]\n",
    "df_gen_target = df.loc[:, 'CAD']\n",
    "\n",
    "# \"Leave one out\" or not\n",
    "leave_one_out = False\n",
    "if leave_one_out:\n",
    "    k = len(df)\n",
    "else:\n",
    "    k = 10\n",
    "    \n",
    "# Splitting data\n",
    "kfold = StratifiedKFold(k, True, 244)\n",
    "sp_data = np.array(list(kfold.split(df, df_target)))\n",
    "sp_gen_data = np.array(list(kfold.split(df, df_gen_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train_model(model, kfold_split_indexes, df_data, df_target):\n",
    "    results = []\n",
    "    for train_indexes, test_indexes in kfold_split_indexes:\n",
    "        x_train = df_data.iloc[train_indexes, :]\n",
    "        y_train = df_target.iloc[train_indexes]\n",
    "        x_test = df_data.iloc[test_indexes, :]\n",
    "        y_test =  df_target.iloc[test_indexes]\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict(x_test)\n",
    "        results.append(accuracy_score(y_test, predictions))\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7533801234511384 0.03522601899041005\n",
      "0.7329443058353171 0.024334465150684957\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "res = kfold_train_model(log_reg, sp_data, df_data, df_target)\n",
    "res_gen = kfold_train_model(log_reg, sp_gen_data, df_gen_data, df_gen_target)\n",
    "print(res.mean(), res.std())\n",
    "print(res_gen.mean(), res_gen.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7471356074131723 0.02354989941157972\n",
      "0.670501625160227 0.0360898506931481\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "res = kfold_train_model(rf, sp_data, df_data, df_target)\n",
    "res_gen = kfold_train_model(rf, sp_gen_data, df_gen_data, df_gen_target)\n",
    "print(res.mean(), res.std())\n",
    "print(res_gen.mean(), res_gen.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581044375267046 0.02931638177792102\n",
      "0.721255455350058 0.024943828169692817\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "res = kfold_train_model(gb, sp_data, df_data, df_target)\n",
    "res_gen = kfold_train_model(gb, sp_gen_data, df_gen_data, df_gen_target)\n",
    "print(res.mean(), res.std())\n",
    "print(res_gen.mean(), res_gen.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7619681987425991 0.032143204995460674\n",
      "0.721255455350058 0.024943828169692817\n"
     ]
    }
   ],
   "source": [
    "# Naive bayess\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "res = kfold_train_model(gnb, sp_data, df_data, df_target)\n",
    "res_gen = kfold_train_model(gb, sp_gen_data, df_gen_data, df_gen_target)\n",
    "print(res.mean(), res.std())\n",
    "print(res_gen.mean(), res_gen.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
